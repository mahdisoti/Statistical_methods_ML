# Ensemble methods are a way of combining the predictions of multiple classifiers to get a better answer than simply using one classifier. Combining multiple classifiers can also solve the problems of single classifiers, such as overfitting. We used the popular variant of boosting, called AdaBoost. AdaBoost uses a bunch of weak learners as the base classifier with the input data weighted by a weight vector. We use functions to create a classifier using AdaBoost and the weak learner, decision stumps(which is a single-node tree). This decision stump will later feed our Adaboost Algorithm. The dataset that we are going to apply all of this is “Forest Cover Type Dataset” where each row includes variables like tree type, shadow coverage, distance to nearby landmarks, soil type, local topography, etc. What we will try to do is to predict the cover type of the forest based among 7 different possible classes. The cover type labels has 7 classes, to address this we use a one-vs-all-encoding method. Next, we use a 5-fold cross validation to evaluate the multiclass classification performance for different values of the Adaboost iterations. After evaluating different values for Adaboost  rounds 50,150,250,350, and 450, since we don’t see much difference in the performance, and considering the computational power, reasonability, and scalability of the solution, we tune our final model with 150 Adaboost rounds and achieve around 72% test and 73% train accuracy.
